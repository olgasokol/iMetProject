\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{{./images/}}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Solving iMet: Fine-grained Multi-label Art Objects Classification with Image-Label Embeddings}

\author{Ekaterina Arkhangelskaia, Olha Sokol\\
Universit{\"a}t des Saarlandes\\
Saarbr{\"u}cken, Germany\\
{\tt\small \{s8ekarkh,s8olsoko\}@stud.uni-saarland.de }
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   People have been collecting art pieces even before our era. Ever since, the amount of art pieces have only been growing. With this increasing growth there is a demand in categorizing and labeling art data. Using manual labor for this purpose can be extremely expensive, therefore an automated solution is needed. Previously, there were some attempts at automated labeling of paintings \cite{Rijks}, however little research have been made in labeling different types of art, like sculpture, ceramics or applied arts. 
   In this project we attempt to perform multi-label classification task on iMet dataset. We utilize label embeddings to improve accuracy of fine-tuned ResNet18 network. Futhermore we use t-SNE algorithm to visualize learned features on artistic data. 

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Art has always been a part of human's life. There are more than 55000 museums in the world, and due to rapid digitilizing of the world a lot of people such as researchers or other scholars would benefit from ability to quickly  navigate among all the data they contain. However, it can be time consuming and inefficient to label all the data by hand. This task motivates us to build end-to-end system to provide automatic labels for sparce and semantically different artistic data. We believe that describing finer-grained attributes from the museum-goer’s understanding, adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects.

\subsection{Dataset}
%\includegraphics[width=\textwidth]{banner_imet}
\begin{figure}[h!]
\begin{center}
    \includegraphics[width=0.25\textwidth]{sample}
\end{center}
  \caption{Sample image from dataset. Culture: central european. Tags: landscapes, lovers, men, trees, woman}
  \label{fig:imet_sample}
\end{figure}
We use iMet dataset from iMet Collection 2019 - FGVC6 challenge \cite{imet}. It is a collection of more than 109k images of different art pieces collected at Metropolitan Museum of Art in New York, also known as The Met.
Each data sample contains one image and at least one attribute label from a label set of 1103 attributes, as can bee seen on Fig \ref{fig:imet_sample}. The dimension of each image is normalized such that the shorter dimension is 300 pixels. Provided annotations are considered somewhat noisy, as they were provided by single worker without verification step. Annotations are devided into 2 categories, culture and tags, tags being related to what is “seen” or what is infered as the object’s “utility.”	

\subsection{Problem statement}
\begin{figure}[h!]
  \includegraphics[width=0.5\textwidth]{labels_frequency}
  \caption{Top-30 labels by frequency}
  \label{fig:top30}
\end{figure}


As an input we use single image from iMet dataset, passing it through a neural net we want to receive a set of labels.
As we can see from Fig. \ref{fig:top30}, labels for this dataset are very unbalanced, making training conventional CNNs very challenging. To adress this issue we use labels embeddings to encourage labels close in embedding space to appear together more often.

\subsection{Related work}

There's been a lot of work in classification paintings by author, learning their artistic style  \cite{paint_class}, \cite{rec_style}, \cite{learned_style}, \cite{draw_style}, \cite{la_style}, \cite{la_style_2}, and style transfer \cite{johnson_style_transf}, \cite{huang_style_transf}, \cite{ulyaniv_stylized_img}, \cite{jie_style_transf}. However, little has been made in classification of art pieces that are not paintings. 

T. Balakrishan et al. in \cite{Rijks} tried to classify paintings of Rijksmuseum by their respective authors. To adress imbalance problem they reduces their authors set to include only top 10 artist regarding amount of paintings they produced. Then they used pretrained ResNet-18 and fine-tuned its last layer. However with such architercture they encountered accuracy drop by 0.3\% when increasing number of classes from 10 to 20.

M. Sabatelli et al. in \cite{deep_cnn_class} try to solve different classification problems on datasets similar to ours. Thy try to classify heritage art pieces from Rijksmuseum and Antwerp datasets according to their author, art style and material. To tackle this problem they use 4 architectures: VGG19 \cite{vgg}, Inception-V3 \cite{inc-v3}, Xception \cite{xception} and ResNet50 \cite{resnet50}. All of mentioned neural nets are fine-tuned with SGD in conjunction with early stopping mechanism. Categorical crossentropy loss function was used for all 3 multi-class classification problem. They also confirm idea that art features learned on Rijksmuseum dataset can be transferred on Antwerp dataset. 

Akata et al. in \cite{akata_emb} argue that attribute based image classification can be viewed as label-embedding problem. In their setup each class is embedded in the space of attribute vectors and function to optimizen is defined as 
$$f(x,w)=\underset{y \in Y}{\mathrm{argmax }}\ F(x,y;w)$$
where $w$ is model parameters and $F(x,y;w)$ measures how compatible is pair $(x,y)$ giwen $w$. They also assume that $F$ is linear in some joint feature embedding $\psi(x,y)$:
$$ F(x,y;w)=w'\psi(x,y)$$
and embedding $\psi$ can be rewritten as tensor product of image features $\theta : \mathcal{X} \rightarrow \widetilde{\mathcal{X}} = \mathbb{R}^D$ and label embedding $\varphi : \mathcal{Y} \rightarrow \widetilde{\mathcal{Y}} = \mathbb{R}^E$:
$$\psi(x,y)=\theta(x)\oplus \varphi(y). $$
To optimize $f(x,w)$ directly image classification and not only compatibility between input and output embeddings authors adapt WSABIE algorithm \cite{wsabie}. This setup proves to overcome 3 common problems existing in attribute learning. First, as mentioned before, they don't introduce intermediate problem of learning attribute classification. Then, they use training data to update label embedding by using as prior attribute embedding. Futhermore, their method can utilize other sources of additional information such as word embeddings derived from text or class hierarchies.
%-------------------------------------------------------------------------

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
\begin{thebibliography}{9}

\bibitem{Rijks} 
Tara Balakrishan, Sarah Rosston, Emily Tang
\textit{Using CNN to Classify and Understand Artists from the Rijksmuseum}.
2017. Available at: \texttt{http://cs231n.stanford.edu/reports/2017/ pdfs/410.pdf}

\bibitem{deep_cnn_class} 
Matthia Sabatelli, Mike Kestemont, Walter Daelemans, Pierre Geurts
\textit{Deep Transfer Learning for Art Classification Problems}.
ECCV VisArt Workshop on Computer Vision for Art Analysis (September 2018, Munich GE).

\bibitem{akata_emb}
Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid. 
\textit{Label-Embedding for Image Classification}
arXiv preprint arXiv:1503.08677 [cs.CV]

\bibitem{imet} 

\textit{iMet Collection}. 
 FGVC6, CVPR 2019.

\bibitem{corr_feat}
Wei-Ta Chu, Yi-Ling Wu.
\textit{Deep Correlation Features for Image Style Classification.}
ACM Multimedia 2016.

\bibitem{rec_style}
Sergey Karayev, Aaron Hertzmann, Holger Winnemöller, Aseem Agarwala, Trevor Darrell.
\textit{Recognizing Image Style.}
BMVC 2013.

\bibitem{learned_style}
Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur.
\textit{A Learned Representation For Artistic Style.}
ICLR 2016.

\bibitem{paint_class}
Wei Ren Tan, Chee Seng Chan, Hernán E. Aguirre, Kiyoshi Tanaka.
\textit{Ceci n'est pas une pipe: A deep convolutional network for fine-art paintings classification.}
IEEE International Conference on Image Processing (ICIP).

\bibitem{draw_style}
Samet Hicsonmez, Nermin Samet, Fadime Sener, Pinar Duygulu.
\textit{DRAW: Deep networks for Recognizing styles of Artists Who illustrate children's books.}
Proceedings of ICMR ’17, June 6–9, 2017, Bucharest, Romania.

\bibitem{la_style}
L. A. Gatys, A. S. Ecker, and M. Bethge.
\textit{A neural algorithm of artistic style.}
arXiv preprint arXiv:1508.06576, 2015.

\bibitem{la_style_2}
L. A. Gatys, A. S. Ecker, and M. Bethge.
\textit{Image style transfer using convolutional neural networks.}
In CVPR, 2016.

\bibitem{johnson_style_transf}
J. Johnson, A. Alahi, and L. Fei-Fei.
\textit{ Perceptual losses for real-time style transfer and super-resolution.}
In ECCV, 2016.

\bibitem{huang_style_transf}
X. Huang and S. J. Belongie.
\textit{Arbitrary style transfer in real-time with adaptive instance normalization.}
In ICCV, 2017.

\bibitem{ulyaniv_stylized_img}
D. Ulyanov, V. Lebedev, A. Vedaldi, and V. S. Lempitsky.
\textit{Texture networks: feed-forward synthesis of textures and stylized images.}
In ICML, 2016.

\bibitem{jie_style_transf}
Jie An, Haoyi Xiong, Jiebo Luo, Jun Huan, Jinwen Ma.
\textit{Fast Universal Style Transfer for Artistic and Photorealistic Rendering}
arXiv preprint arXiv:1907.03118v1, 2019

\bibitem{vgg}
Simonyan, K., Zisserman, A.
\textit{Very deep convolutional networks for large-scale image recognition.}
arXiv preprint arXiv:1409.1556 (2014)

\bibitem{inc-v3}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.
\textit{Rethinking the inception architecture for computer vision.}
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2818–2826 (2016)

\bibitem{resnet50}
Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.
\textit{Aggregated residual transformations for deep neural networks.}
In: Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. pp. 5987–5995. IEEE (2017)

\bibitem{xception}
Chollet, F.
\textit{Xception: Deep learning with depthwise separable convolutions.}
arXiv preprint (2016)

\bibitem{wsabie}
J. Weston, S. Bengio, and N. Usunier. 
\textit{Large scale image annotation: Learning to rank with joint word-image embeddings.}
ECML, 2010.
\end{thebibliography}
}

\end{document}
